---
title: ''
subtitle: ""
author: ""
date: ""

output:
  pdf_document:
  fig_crop: false
highlight: tango
number_sections: false
fig_caption: true
keep_tex: true
includes:
  in_header: Estilo.sty
classoption: a4paper
always_allow_html: true
---

\begin{center}
{\Large
  DEPARTAMENTO DE ESTATÍSTICA} \\
\vspace{0.5cm}
\begin{figure}[!t]
\centering
\includegraphics[width=9cm, keepaspectratio]{logo-UnB.eps}
\end{figure}
\vskip 1em
{\large
  15 de fevereiro de 2023}
\vskip 3em
{\LARGE
  \textbf{Lista 3: Manipulação e modelagem de dados com Spark}} \\
\vskip 1em
{\LARGE
  \textbf{Resolução - William Rappel - 22/0006032}} \\
\vskip 1em
{\Large
  Computação em Estatística para dados e cálculos massivos} \\
\vskip 1em
{\Large
  Tópicos especiais em Estatística 2} \\
\vskip 3em
{\Large
  Prof. Guilherme Rodrigues} \\
\vskip 1em
{\Large
  César Augusto Fernandes Galvão (aluno colaborador)} \\
\vskip 1em
{\Large
  Gabriel Jose dos Reis Carvalho (aluno colaborador)} \\
\end{center}

\vskip 5em

\begin{enumerate}
\item \textbf{As questões deverão ser respondidas em um único relatório \emph{PDF} ou \emph{html}, produzido usando as funcionalidades do \emph{Rmarkdown} ou outra ferramenta equivalente}.
\item \textbf{O aluno poderá consultar materiais relevantes disponíveis na internet, tais como livros, \emph{blogs} e artigos}.
\item \textbf{O trabalho é individual. Suspeitas de plágio e compartilhamento de soluções serão tratadas com rigor.}
\item \textbf{Os códigos \emph{R} utilizados devem ser disponibilizados na integra, seja no corpo do texto ou como anexo.}
\item \textbf{O aluno deverá enviar o trabalho até a data especificada na plataforma Microsoft Teams.}
\item \textbf{O trabalho será avaliado considerando o nível de qualidade do relatório, o que inclui a precisão das respostas, a pertinência das soluções encontradas, a formatação adotada, dentre outros aspectos correlatos.}
\item \textbf{Escreva seu código com esmero, evitando operações redundantes, visando eficiência computacional, otimizando o uso de memória, comentando os resultados e usando as melhores práticas em programação.}
\end{enumerate}

```{r setup, results=FALSE, message=FALSE, echo=FALSE, warnings=FALSE}
knitr::opts_chunk$set(echo=T, warnings=F)

# carrega os pacotes
if (!require('pacman')) install.packages('pacman')
pacman::p_load(stringr, tidyr, purrr, sparklyr, microbenchmark)
```

\newpage

## Questão 1: Criando o cluster spark.

**a)** Crie uma pasta (chamada datasus) em seu computador e faça o download dos arquivos referentes ao Sistema de informação de Nascidos Vivos (SINASC), os quais estão disponíveis em https://datasus.saude.gov.br/transferencia-de-arquivos/. 

**Atenção:** Considere apenas os Nascidos Vivos no Brasil (sigla DN) entre 1996 e 2020, incluindo os dados estaduais e excluindo os arquivos referentes ao Brasil (sigla BR). Use wi-fi para fazer os downloads!

**Dica:** O endereço ftp://ftp.datasus.gov.br/dissemin/publicos/SINASC/1996_/Dados/DNRES/ permite a imediata identificação dos endereços e arquivos a serem baixados.

\textcolor{red}{\bf Solução}

Primeiro, vamos criar a pasta datasus, caso ela não exista.

```{r create-folder-datasus}
# cria pasta datasus
datasus <- 'datasus/'
if (!file.exists(datasus)) dir.create(datasus)
```

Em seguida, vamos criar uma subpasta dbc, que armazenará os arquivos neste formato.

```{r create-folder-datasus-dbc}
# cria subpasta datasus/dbc
dbc <- 'datasus/dbc/'
if (!file.exists(dbc)) dir.create(dbc)
```

Em seguida, vamos realizar o download dos arquivos requisitados.

```{r download-files}
# objetos utilizados no download
url <- 'ftp://ftp.datasus.gov.br/dissemin/publicos/SINASC/'
ufs <- c('AC', 'AL', 'AM', 'AP', 'BA', 'CE', 'DF', 'ES', 'GO', 
         'MA', 'MG', 'MS', 'MT', 'PA', 'PB', 'PE', 'PI', 'PR',
         'RJ', 'RN', 'RO', 'RR', 'RS', 'SC', 'SE', 'SP', 'TO')
years <- 1996:2020
ufs_years <- do.call(str_c, expand_grid(ufs, years))

# funcao para download dos dados
download_dbc <- function(uf_year) {
  file_name <- str_c(dbc, uf_year, '.dbc')
  if (!file.exists(file_name)) {
    link <- str_c(url, '1996_/Dados/DNRES/DN', uf_year, '.dbc')
    download.file(link, file_name, mode='wb')
  }
}

# execucao do download
walk(.x=ufs_years, .f=download_dbc)
```

**b)** Usando a função `p_load` (do pacote `pacman`), carregue os pacotes `arrow` e `read.dbc` e converta os arquivos baixados no item a) para formato o *.parquet*. Em seguida, converta para *.csv* apenas os arquivos referentes aos estados GO, MS e ES. Considerando apenas os referidos estados, compare o tamanho ocupado pelos arquivos nos formatos *.parquet* e *.csv* (use a função `file.size`).

\textcolor{red}{\bf Solução}

Primeiro, carregamos os pacotes. Em seguida, criamos uma subpasta denominada parquet e convertemos cada arquivo para o formato *.parquet*.

```{r convert-files-parquet}
# carrega pacotes
pacman::p_load(arrow, read.dbc)

# cria subpasta datasus/parquet
parquet <- 'datasus/parquet/'
if (!file.exists(parquet)) dir.create(parquet)

# lista com arquivos dbc
dbc_files <- list.files(dbc, '.dbc', full.names=T)

# funcao para converter para parquet
to_parquet <- function(dbc_file) {
  parquet_file <- str_replace_all(dbc_file, 'dbc', 'parquet')
  if (!file.exists(parquet_file)) {
    data <- read.dbc(dbc_file)
    write_parquet(data, parquet_file)
  }
}

# execucao da conversao
walk(.x=dbc_files, .f=to_parquet)
```

Agora, iremos converter para *.csv* apenas os arquivos referentes aos estados GO, MS e ES.

```{r convert-files-csv}
# cria subpasta datasus/csv
csv <- 'datasus/csv/'
if (!file.exists(csv)) dir.create(csv)

# lista com arquivos parquet dos estados desejados
parquet_files_ESGOMS <- list.files(parquet, 'ES|GO|MS', full.names=T)

# funcao para converter para csv
to_csv <- function(parquet_file) {
  csv_file <- str_replace_all(parquet_file, 'parquet', 'csv')
  if (!file.exists(csv_file)) {
    data <- read_parquet(parquet_file)
    write.csv(data, csv_file)
  }
}

# execucao da conversao
walk(.x=parquet_files_ESGOMS, .f=to_csv)
```

Por último, realizamos a comparação entre os tamanhos ocupados pelos arquivos nos formatos *.parquet* e *.csv*, com a função `file.size`.

```{r file-size-comparison}
# lista com nomes dos arquivos csv
csv_files_ESGOMS <- list.files(csv, 'ES|GO|MS', full.names=T)

# tamanho ocupado csv, em megabytes
(csv_size_mb <- sum(map_dbl(.x=csv_files_ESGOMS, .f=file.size)/1e6))

# tamanho ocupado parquet, em megabytes
(parquet_size_mb <- sum(map_dbl(.x=parquet_files_ESGOMS, .f=file.size)/1e6))
```

Assim, concluímos que os arquivos *.parquet* ocupam um tamanho aproximadamente 10x menor do que *.csv*, para este conjunto de dados.

**c)** Crie uma conexão `Spark`, carregue para ele os dados em formato *.parquet* e *.csv* e compare os respectivos tempos computacionais. Se desejar, importe apenas as colunas necessárias para realizar a Questão 2. 

**OBS:** Lembre-se de que quando indicamos uma pasta na conexão, as colunas escolhidas para a análise precisam existir em todos os arquivos.

\textcolor{red}{\bf Solução}

Primeiro, vamos criamos uma subpasta denominada csv-spark. Vamos ler cada arquivo *.csv*, manter apenas as colunas necessárias para realizar a Questão 2, e salvar um novo arquivo *.csv* nessa subpasta.

```{r filter-columns-csv}
# cria subpasta datasus/csv-spark
csv_spark <- 'datasus/csv-spark/'
if (!file.exists(csv_spark)) dir.create(csv_spark)

# colunas para manter
aux <- read_parquet(str_c(parquet, 'AC1996.parquet'))
cols <- colnames(aux)
cols_keep <- cols[!(cols %in% c('contador', 'CODOCUPMAE'))]

# funcao para ler, filtrar colunas e exportar
filter_csv <- function(csv_file) {
  csv_filtered <- str_replace_all(csv_file, '[/]csv[/]', '/csv-spark/')
  if (!file.exists(csv_filtered)) {
    data <- read.csv(csv_file)
    data <- data[, cols_keep]
    write.csv(data, csv_filtered)
  }
}

# execucao da conversao
walk(.x=csv_files_ESGOMS, .f=filter_csv)
```

Agora, vamos criamos uma subpasta denominada parquet-spark. Vamos ler cada arquivo *.parquet*, manter apenas as colunas necessárias para realizar a Questão 2, e salvar um novo arquivo *parquet.* nessa subpasta.

```{r filter-columns-parquet}
# cria subpasta datasus/parquet-spark
parquet_spark <- 'datasus/parquet-spark/'
if (!file.exists(parquet_spark)) dir.create(parquet_spark)

# funcao para ler, filtrar colunas e exportar
filter_parquet <- function(parquet_file) {
  parquet_filtered <- str_replace_all(parquet_file, '[/]parquet[/]', '/parquet-spark/')
  if (!file.exists(parquet_filtered)) {
    data <- read_parquet(parquet_file)
    data <- data[, cols_keep]
    write_parquet(data, parquet_filtered)
  }
}

# execucao da conversao
walk(.x=parquet_files_ESGOMS, .f=filter_parquet)
```

Em seguida, vamos criar a conexão `Spark`.

```{r spark-connect}
# conexao ao spark
conf <- spark_config()
conf$`sparklyr.cores.local` <- 3
conf$`sparklyr.shell.driver-memory` <- '8G'
conf$spark.memory.fraction <- 0.9
sc <- spark_connect(master='local', config=conf)
```

Agora comparamos os tempos computacionais de envio dos arquivos ao `Spark`.

```{r microbenchmark-spark}
# funcao para enviar csv
microbenchmark_csv <- function() {
  spark_read_csv(
    sc=sc,
    name='ES_GO_MS_csv',
    path=csv_spark,
    memory=F
  )
}

# funcao para enviar parquet
microbenchmark_parquet <- function() {
  spark_read_parquet(
    sc=sc,
    name='ES_GO_MS_parquet',
    path=parquet_spark,
    memory=F
  )
}

# comparacao com o pacote microbenchmark
microbenchmark(
  csv = microbenchmark_csv(),
  parquet = microbenchmark_parquet(),
  times = 3
)
```

Pelo tempo mediano de execução, ao utilizar arquivos *.parquet* tivemos um aumento de 10x na velocidade de envio dos dados ao `Spark`.

Para realizar a questão 2, precisamos que os dados referentes a todos os estados e anos estejam no `Spark`. Para isso, vamos realizar procedimento semelhante ao feito anteriormente, porém agora com todos os 27 estados/distritos.

```{r send-parquet-spark}
# cria subpasta datasus/parquet-spark-full
parquet_spark_full <- 'datasus/parquet-spark-full/'
if (!file.exists(parquet_spark_full)) dir.create(parquet_spark_full)

# lista com arquivos parquet
parquet_files <- list.files(parquet, full.names=T)

# funcao para ler, filtrar colunas e exportar
filter_parquet_full <- function(parquet_file) {
  parquet_filtered <- str_replace_all(parquet_file, '[/]parquet[/]', '/parquet-spark-full/')
  if (!file.exists(parquet_filtered)) {
    data <- read_parquet(parquet_file)
    data <- data[, cols_keep]
    data$UF <- str_extract(parquet_file, '[A-Z]{2}')
    data$ANO <- str_extract(parquet_file, '[0-9]{4}')
    write_parquet(data, parquet_filtered)
  }
}

# execucao da conversao
walk(.x=parquet_files, .f=filter_parquet_full)

# envio ao spark
spark_read_parquet(
  sc=sc,
  name='tab_full',
  path=parquet_spark_full,
  memory=F
)
```

## Questão 2: Preparando e modelando os dados.

**Atenção**: **Elabore seus comandos dando preferência as funcionalidades do pacote sparklyr**.

**a)** Faça uma breve análise exploratória dos dados (tabelas e gráficos) com base somente nas colunas existente nos arquivos de 1996. O dicionário das variaveis encontra-se no mesmo site do item a), na parte de documentação. Corrija eventuais erros encontrados; por exemplo, na variavel sexo são apresentados rótulos distintos para um mesmo significado.

\textcolor{red}{\bf Solução}


**b)** Ultilizando as funções do **sparklyr**, preencha os dados faltantes na idade da mãe com base na mediana. Se necessário, faça imputação de dados também nas demais váriaveis.

<!-- \textcolor{red}{\bf Solução} -->

**c)** Novamente, ultilizando as funções do **sparklyr**, normalize (retire a média e divida pelo desvio padrão) as variáveis quantitativas do banco.

<!-- \textcolor{red}{\bf Solução} -->

**d)** Crie variáveis dummy (*one-hot-encoding*) que conjuntamente indiquem o dia da semana do nascimento (SEG, TER, ...). Em seguida, *binarize* o número de consultas pré-natais de modo que "0" represente "até 5 consultas" e "1" indique "6 ou mais consultas". (Ultilize as funções **ft_**)

<!-- \textcolor{red}{\bf Solução} -->



<!-- \textcolor{red}{\bf Solução} -->

**e)** Particione os dados aleatoriamente em bases de treinamento e teste. Ajuste, sobre a base de treinamento, um modelo de regressão logistica em que a variável resposta (*y*), indica se o parto foi ou não cesáreo. Analise o desempenho preditivo do modelo com base na matrix de confusão obtida no conjunto de teste. 

<!-- \textcolor{red}{\bf Solução} -->






```{r}
spark_disconnect(sc)
```